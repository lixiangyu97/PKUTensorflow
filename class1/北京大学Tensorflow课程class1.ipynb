{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df20a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a100da",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "lr = 0.2  #学习率\n",
    "epoch = 40  #轮数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cabb4ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ae0de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 40)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d898fda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch,w is 2.600000,loss is 36.000000\n",
      "After 1 epoch,w is 1.160000,loss is 12.959999\n",
      "After 2 epoch,w is 0.296000,loss is 4.665599\n",
      "After 3 epoch,w is -0.222400,loss is 1.679616\n",
      "After 4 epoch,w is -0.533440,loss is 0.604662\n",
      "After 5 epoch,w is -0.720064,loss is 0.217678\n",
      "After 6 epoch,w is -0.832038,loss is 0.078364\n",
      "After 7 epoch,w is -0.899223,loss is 0.028211\n",
      "After 8 epoch,w is -0.939534,loss is 0.010156\n",
      "After 9 epoch,w is -0.963720,loss is 0.003656\n",
      "After 10 epoch,w is -0.978232,loss is 0.001316\n",
      "After 11 epoch,w is -0.986939,loss is 0.000474\n",
      "After 12 epoch,w is -0.992164,loss is 0.000171\n",
      "After 13 epoch,w is -0.995298,loss is 0.000061\n",
      "After 14 epoch,w is -0.997179,loss is 0.000022\n",
      "After 15 epoch,w is -0.998307,loss is 0.000008\n",
      "After 16 epoch,w is -0.998984,loss is 0.000003\n",
      "After 17 epoch,w is -0.999391,loss is 0.000001\n",
      "After 18 epoch,w is -0.999634,loss is 0.000000\n",
      "After 19 epoch,w is -0.999781,loss is 0.000000\n",
      "After 20 epoch,w is -0.999868,loss is 0.000000\n",
      "After 21 epoch,w is -0.999921,loss is 0.000000\n",
      "After 22 epoch,w is -0.999953,loss is 0.000000\n",
      "After 23 epoch,w is -0.999972,loss is 0.000000\n",
      "After 24 epoch,w is -0.999983,loss is 0.000000\n",
      "After 25 epoch,w is -0.999990,loss is 0.000000\n",
      "After 26 epoch,w is -0.999994,loss is 0.000000\n",
      "After 27 epoch,w is -0.999996,loss is 0.000000\n",
      "After 28 epoch,w is -0.999998,loss is 0.000000\n",
      "After 29 epoch,w is -0.999999,loss is 0.000000\n",
      "After 30 epoch,w is -0.999999,loss is 0.000000\n",
      "After 31 epoch,w is -1.000000,loss is 0.000000\n",
      "After 32 epoch,w is -1.000000,loss is 0.000000\n",
      "After 33 epoch,w is -1.000000,loss is 0.000000\n",
      "After 34 epoch,w is -1.000000,loss is 0.000000\n",
      "After 35 epoch,w is -1.000000,loss is 0.000000\n",
      "After 36 epoch,w is -1.000000,loss is 0.000000\n",
      "After 37 epoch,w is -1.000000,loss is 0.000000\n",
      "After 38 epoch,w is -1.000000,loss is 0.000000\n",
      "After 39 epoch,w is -1.000000,loss is 0.000000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch):  # for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。\n",
    "    with tf.GradientTape() as tape:  # with结构到grads框起了梯度的计算过程。\n",
    "        loss = tf.square(w + 1)\n",
    "    grads = tape.gradient(loss, w)  # .gradient函数告知谁对谁求导\n",
    "\n",
    "    w.assign_sub(lr * grads)  # .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads\n",
    "    print(\"After %s epoch,w is %f,loss is %f\" % (epoch, w.numpy(), loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d61d1a",
   "metadata": {},
   "source": [
    "## 创建一个张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e909d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant([1,5],dtype = tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396189d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 5], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af4b0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c395674",
   "metadata": {},
   "source": [
    "### numpy类型转化为Tensor类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04b3d28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.arange(0,5)  ##一维数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8150fd17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c189053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.convert_to_tensor(a,dtype = tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3397cf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([0, 1, 2, 3, 4], dtype=int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8519906",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = [[8 for i in range(1,5)]for j in range(1,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6695c90c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 8, 8, 8], [8, 8, 8, 8], [8, 8, 8, 8], [8, 8, 8, 8]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c84f562a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tensor = tf.convert_to_tensor(num,dtype = tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "034bfc14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4), dtype=int64, numpy=\n",
       "array([[8, 8, 8, 8],\n",
       "       [8, 8, 8, 8],\n",
       "       [8, 8, 8, 8],\n",
       "       [8, 8, 8, 8]], dtype=int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0558696",
   "metadata": {},
   "source": [
    "### 创建特殊张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "149b832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建全为0的张量\n",
    "tfzero = tf.zeros(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80dde2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0. 0.], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tfzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61a3fd31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf5fb745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=float32, numpy=\n",
       "array([[[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ec88c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[8, 8, 8],\n",
       "       [8, 8, 8]])>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##创建全为指定值的张量\n",
    "tf.fill([2,3],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfe81530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy=\n",
       "array([[[8, 8, 8, 8],\n",
       "        [8, 8, 8, 8],\n",
       "        [8, 8, 8, 8]],\n",
       "\n",
       "       [[8, 8, 8, 8],\n",
       "        [8, 8, 8, 8],\n",
       "        [8, 8, 8, 8]]])>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.fill([2,3,4],8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc3101ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##创建全为1的张量\n",
    "tf.ones([2,3])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c4ade96e",
   "metadata": {},
   "source": [
    "维度：\n",
    "一维直接写个数\n",
    "二维用[行，列]\n",
    "多维用[n,m,j,k……]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7f7fc4",
   "metadata": {},
   "source": [
    "### 创建随机张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a80bd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n",
       "array([[0.8687111 , 0.7280222 , 1.3095992 , 0.6352563 , 1.6976922 ,\n",
       "        0.65681195],\n",
       "       [1.9660318 , 1.9209023 , 1.1528186 , 0.5148672 , 2.0997837 ,\n",
       "        1.4718223 ],\n",
       "       [1.0332636 , 1.0223001 , 1.0823501 , 1.2220198 , 1.3255352 ,\n",
       "        0.4562254 ]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#生成正态分布的随机数\n",
    "tf.random.normal((3,6),mean = 1,stddev = 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3385bb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n",
       "array([[1.7789979 , 0.24370605, 0.4774139 , 1.9273791 , 0.3960715 ,\n",
       "        1.0412036 ],\n",
       "       [1.0610424 , 0.7013988 , 1.5516621 , 0.89710814, 1.479861  ,\n",
       "        0.52383745],\n",
       "       [1.3991013 , 1.3390796 , 1.8924996 , 0.514809  , 0.88502324,\n",
       "        0.2609244 ]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal((3,6),mean = 1,stddev = 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d7debb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 6), dtype=float32, numpy=\n",
       "array([[1.5265801 , 0.22174299, 0.80934024, 1.7963563 , 1.3624451 ,\n",
       "        0.916706  ],\n",
       "       [0.33590943, 1.2502735 , 1.4478196 , 1.3912369 , 1.5414166 ,\n",
       "        1.934449  ],\n",
       "       [0.5330298 , 1.3450794 , 0.7716276 , 2.4402342 , 0.15875071,\n",
       "        1.5022037 ]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal((3,6),mean = 1,stddev = 0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2afd876e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[-0.09289569,  0.4187469 ],\n",
       "       [-0.39141405, -0.7799436 ]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#生成截断式正态分布的随机数\n",
    "tf. random.truncated_normal([2, 2], mean=0.5, stddev=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8958c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成均匀分布随机数\n",
    "f = tf.random.uniform([2, 2], minval=0, maxval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19c9f7e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[0.18627787, 0.06314552],\n",
       "       [0.85242474, 0.72225213]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe343ce9",
   "metadata": {},
   "source": [
    "## Tensorflow 常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b7b7280",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.constant([1,2,3],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6dfc6b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([1., 2., 3.], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "258a0b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float64, numpy=array([1., 2., 3.])>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 强制转换函数\n",
    "tf.cast(x1,dtype = tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb3c0e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算张量维度上元素的最小值\n",
    "tf.reduce_min(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6dc665d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 计算张量维度上元素的最大值\n",
    "tf.reduce_max(x1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0c46049",
   "metadata": {},
   "source": [
    "axis 表示操作方向\n",
    "axis = 0 表示纵向操作，跨行操作\n",
    "axis = 1 表示横向操作，跨列操作\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20c93ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.constant( [ [ 1, 2, 3],[ 2, 2, 3] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c791e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[1, 2, 3],\n",
       "       [2, 2, 3]])>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3327b008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2])>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b08f1054",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3])>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(x,axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f4270",
   "metadata": {},
   "source": [
    "### 标记为可训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bde109b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.ones([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "95d4c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_var = tf.Variable(tf.ones([2,3])) \n",
    "# 标记为可训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c33da6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random.normal([2,2],mean = 0,stddev = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a2832834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
       "array([[ 0.16397308, -1.490126  ],\n",
       "       [ 0.791666  , -0.16641156]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9759df49",
   "metadata": {},
   "source": [
    "### 四则运算"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eab9d3b8",
   "metadata": {},
   "source": [
    "对应元素的四则运算：tf.add，tf.subtract，tf.multiply，tf.divide\n",
    "\n",
    "平方、次方与开方：tf.square，tf.pow，tf.sqrt\n",
    "\n",
    "矩阵乘：tf.matmul"
   ]
  },
  {
   "cell_type": "raw",
   "id": "373d18da",
   "metadata": {},
   "source": [
    "实现两个张量的对应元素相加\n",
    "tf.add(张量1，张量2)\n",
    "\n",
    "实现两个张量的对应元素相减\n",
    "tf.subtract(张量1，张量2)\n",
    "\n",
    "实现两个张量的对应元素相乘\n",
    "tf.multiply(张量1，张量2)\n",
    "\n",
    "实现两个张量的对应元素相除\n",
    "tf.divide(张量1，张量2)\n",
    "\n",
    "PS:只有维度相同的张量才可以做四则运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd84095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.ones([1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aff17acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c138fd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.fill([1,3],3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "906a6471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[3., 3., 3.]], dtype=float32)>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cbc5f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[4., 4., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.add(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bca8524d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a+b: tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)\n",
      "a-b: tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)\n",
      "a*b: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n",
      "b/a: tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"a+b:\", tf.add(a, b))\n",
    "print(\"a-b:\", tf.subtract(a, b))\n",
    "print(\"a*b:\", tf.multiply(a, b))\n",
    "print(\"b/a:\", tf.divide(b, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944efd4",
   "metadata": {},
   "source": [
    "### 平方、次方与开方运算"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46f96d77",
   "metadata": {},
   "source": [
    "计算某个张量的平方\n",
    "tf.square(张量名)\n",
    "\n",
    "计算某个张量的n次方\n",
    "tf.pow(张量名，n次方数)\n",
    "\n",
    "计算某个张量的开方\n",
    "tf.sqrt(张量名）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f707e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.fill([1, 2], 3.)\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a8ca693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[27. 27.]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[9. 9.]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[1.7320508 1.7320508]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.pow(a, 3))\n",
    "print(tf.square(a))\n",
    "print(tf.sqrt(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0a5e3",
   "metadata": {},
   "source": [
    "### 矩阵运算"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dd7abe2",
   "metadata": {},
   "source": [
    "实现两个矩阵的相乘\n",
    "tf.matmul(矩阵1，矩阵2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fa430e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]], shape=(3, 2), dtype=float32)\n",
      "b: tf.Tensor(\n",
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]], shape=(2, 3), dtype=float32)\n",
      "a*b: tf.Tensor(\n",
      "[[6. 6. 6.]\n",
      " [6. 6. 6.]\n",
      " [6. 6. 6.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.ones([3, 2])\n",
    "b = tf.fill([2, 3], 3.)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a*b:\", tf.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5178f52d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: tf.Tensor(\n",
      "[[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]], shape=(3, 2), dtype=float32)\n",
      "b: tf.Tensor(\n",
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]], shape=(2, 3), dtype=float32)\n",
      "a*b: tf.Tensor(\n",
      "[[6. 6. 6.]\n",
      " [6. 6. 6.]\n",
      " [6. 6. 6.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"a:\", a)\n",
    "print(\"b:\", b)\n",
    "print(\"a*b:\", tf.matmul(a, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c976841",
   "metadata": {},
   "source": [
    "### 其他函数"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a94d362",
   "metadata": {},
   "source": [
    "tf.data.Dataset.from_tensor_slices 用于将输入特征和标签配对 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a019e3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = tf.constant([12,23,10,17])\n",
    "labels = tf.constant([0,1,1,0])\n",
    "dataset = tf.data.Dataset.from_tensor_slices((features,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b20ea585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "01287074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int32, numpy=12>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=23>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=10>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=17>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for element in dataset:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f288891",
   "metadata": {},
   "source": [
    "tf.GradientTape 求导运算"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d887ad2c",
   "metadata": {},
   "source": [
    "with结构记录计算过程，gradient求出张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6e02fa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    x = tf.Variable(tf.constant(3.0))\n",
    "    y = tf.pow(x, 2)\n",
    "grad = tape.gradient(y, x)\n",
    "print(grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35b0a2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x1fb369c6e80>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = ['one', 'two', 'three']\n",
    "enumerate(seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "426ad9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 one\n",
      "1 two\n",
      "2 three\n"
     ]
    }
   ],
   "source": [
    "for i, element in enumerate(seq):\n",
    "    print(i, element)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "888bb16a",
   "metadata": {},
   "source": [
    "独热编码tf.one_hot(labels, depth=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "477a79d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 3 #三分类\n",
    "labels = tf.constant([1,0,3])\n",
    "output = tf.one_hot(labels, depth=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abea36c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f009ab4",
   "metadata": {},
   "source": [
    "tf.nn.softmax函数：使得多分类符合概率分布，把评分换算成概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "39558700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After softmax, y_prois: tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "y = tf.constant( [1.01, 2.01, -0.66] )\n",
    "y_pro= tf.nn.softmax(y)\n",
    "print(\"After softmax, y_prois:\", y_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0c6342f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n",
      " [[1 2 3]\n",
      " [2 3 4]\n",
      " [5 4 3]\n",
      " [8 7 2]]\n",
      "每一列的最大值的索引： tf.Tensor([3 3 1], shape=(3,), dtype=int64)\n",
      "每一行的最大值的索引 tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])\n",
    "print(\"test:\\n\", test)\n",
    "print(\"每一列的最大值的索引：\", tf.argmax(test, axis=0))  # 返回每一列最大值的索引\n",
    "print(\"每一行的最大值的索引\", tf.argmax(test, axis=1))  # 返回每一行最大值的索引\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baf3779",
   "metadata": {},
   "source": [
    "## 鸢尾花数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2a0c55df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "059b53f2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data from datasets: \n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n",
      "y_data from datasets: \n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "x_data =load_iris().data  # .data返回iris数据集所有输入特征\n",
    "y_data =load_iris().target  # .target返回iris数据集所有标签\n",
    "print(\"x_data from datasets: \\n\", x_data)\n",
    "print(\"y_data from datasets: \\n\", y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8c0f6b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data =pd.DataFrame(x_data, columns=['花萼长度', '花萼宽度', '花瓣长度', '花瓣宽度']) # 为表格增加行索引（左侧）和列标签（上方）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0695b908",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>花萼长度</th>\n",
       "      <th>花萼宽度</th>\n",
       "      <th>花瓣长度</th>\n",
       "      <th>花瓣宽度</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     花萼长度  花萼宽度  花瓣长度  花瓣宽度\n",
       "0     5.1   3.5   1.4   0.2\n",
       "1     4.9   3.0   1.4   0.2\n",
       "2     4.7   3.2   1.3   0.2\n",
       "3     4.6   3.1   1.5   0.2\n",
       "4     5.0   3.6   1.4   0.2\n",
       "..    ...   ...   ...   ...\n",
       "145   6.7   3.0   5.2   2.3\n",
       "146   6.3   2.5   5.0   1.9\n",
       "147   6.5   3.0   5.2   2.0\n",
       "148   6.2   3.4   5.4   2.3\n",
       "149   5.9   3.0   5.1   1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "76cb844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.unicode.east_asian_width', True)   # 设置列名对齐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a7c36b1f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将特征加入到索引: \n",
      "      花萼长度  花萼宽度  花瓣长度  花瓣宽度\n",
      "0         5.1       3.5       1.4       0.2\n",
      "1         4.9       3.0       1.4       0.2\n",
      "2         4.7       3.2       1.3       0.2\n",
      "3         4.6       3.1       1.5       0.2\n",
      "4         5.0       3.6       1.4       0.2\n",
      "..        ...       ...       ...       ...\n",
      "145       6.7       3.0       5.2       2.3\n",
      "146       6.3       2.5       5.0       1.9\n",
      "147       6.5       3.0       5.2       2.0\n",
      "148       6.2       3.4       5.4       2.3\n",
      "149       5.9       3.0       5.1       1.8\n",
      "\n",
      "[150 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"将特征加入到索引: \\n\", x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5ba83229",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>花萼长度</th>\n",
       "      <th>花萼宽度</th>\n",
       "      <th>花瓣长度</th>\n",
       "      <th>花瓣宽度</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     花萼长度  花萼宽度  花瓣长度  花瓣宽度\n",
       "0         5.1       3.5       1.4       0.2\n",
       "1         4.9       3.0       1.4       0.2\n",
       "2         4.7       3.2       1.3       0.2\n",
       "3         4.6       3.1       1.5       0.2\n",
       "4         5.0       3.6       1.4       0.2\n",
       "..        ...       ...       ...       ...\n",
       "145       6.7       3.0       5.2       2.3\n",
       "146       6.3       2.5       5.0       1.9\n",
       "147       6.5       3.0       5.2       2.0\n",
       "148       6.2       3.4       5.4       2.3\n",
       "149       5.9       3.0       5.1       1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c057d9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data['类别'] = y_data  # 新加一列，列标签为‘类别’，数据为y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "92f13624",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>花萼长度</th>\n",
       "      <th>花萼宽度</th>\n",
       "      <th>花瓣长度</th>\n",
       "      <th>花瓣宽度</th>\n",
       "      <th>类别</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     花萼长度  花萼宽度  花瓣长度  花瓣宽度  类别\n",
       "0         5.1       3.5       1.4       0.2     0\n",
       "1         4.9       3.0       1.4       0.2     0\n",
       "2         4.7       3.2       1.3       0.2     0\n",
       "3         4.6       3.1       1.5       0.2     0\n",
       "4         5.0       3.6       1.4       0.2     0\n",
       "..        ...       ...       ...       ...   ...\n",
       "145       6.7       3.0       5.2       2.3     2\n",
       "146       6.3       2.5       5.0       1.9     2\n",
       "147       6.5       3.0       5.2       2.0     2\n",
       "148       6.2       3.4       5.4       2.3     2\n",
       "149       5.9       3.0       5.1       1.8     2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0fc4dfcf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加入一列后\n",
      "      花萼长度  花萼宽度  花瓣长度  花瓣宽度  类别\n",
      "0         5.1       3.5       1.4       0.2     0\n",
      "1         4.9       3.0       1.4       0.2     0\n",
      "2         4.7       3.2       1.3       0.2     0\n",
      "3         4.6       3.1       1.5       0.2     0\n",
      "4         5.0       3.6       1.4       0.2     0\n",
      "..        ...       ...       ...       ...   ...\n",
      "145       6.7       3.0       5.2       2.3     2\n",
      "146       6.3       2.5       5.0       1.9     2\n",
      "147       6.5       3.0       5.2       2.0     2\n",
      "148       6.2       3.4       5.4       2.3     2\n",
      "149       5.9       3.0       5.1       1.8     2\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(\"加入一列后\\n\", x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00e92c",
   "metadata": {},
   "source": [
    "## 神经网络实现鸢尾花分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fcf3e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5edb2324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据，分别为输入特征和标签\n",
    "x_data =load_iris().data\n",
    "y_data =load_iris().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "60c36e06",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3df6a7ff",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3be870da",
   "metadata": {},
   "source": [
    "# 随机打乱数据（因为原始数据是顺序的，顺序不打乱会影响准确率）\n",
    "# seed: 随机数种子，是一个整数，当设置之后，每次生成的随机数都一样（为方便教学，以保每位同学结果一致）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b20429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(116) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e9a82e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(np.random.seed(116) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c15b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(116)  # 使用相同的seed，保证输入特征和标签一一对应\n",
    "np.random.shuffle(x_data)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_data)\n",
    "tf.random.set_seed(116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c07ccb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将打乱后的数据集分割为训练集和测试集，训练集为前120行，测试集为后30行\n",
    "x_train = x_data[:-30]\n",
    "y_train = y_data[:-30]\n",
    "x_test = x_data[-30:]\n",
    "y_test = y_data[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "42ed9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换x的数据类型，否则后面矩阵相乘时会因数据类型不一致报错\n",
    "x_train = tf.cast(x_train, tf.float32)\n",
    "x_test = tf.cast(x_test, tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "baa0efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from_tensor_slices函数使输入特征和标签值一一对应。（把数据集分批次，每个批次batch组数据）\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "03ba75a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 4), (None,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f57cee2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((None, 4), (None,)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c5977ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成神经网络的参数，4个输入特征故，输入层为4个输入节点；因为3分类，故输出层为3个神经元\n",
    "# 用tf.Variable()标记参数可训练\n",
    "# 使用seed使每次生成的随机数相同（方便教学，使大家结果都一致，在现实使用时不写seed）\n",
    "w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev=0.1, seed=1))\n",
    "b1 = tf.Variable(tf.random.truncated_normal([3], stddev=0.1, seed=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "639f00fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1  # 学习率为0.1\n",
    "train_loss_results = []  # 将每轮的loss记录在此列表中，为后续画loss曲线提供数据\n",
    "test_acc = []  # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据\n",
    "epoch = 500  # 循环500轮\n",
    "loss_all = 0  # 每轮分4个step，loss_all记录四个step生成的4个loss的和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3b31de26",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 0.2821310982108116\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 1, loss: 0.25459614023566246\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 2, loss: 0.22570250183343887\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 3, loss: 0.21028400212526321\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 4, loss: 0.19942265003919601\n",
      "Test_acc: 0.16666666666666666\n",
      "--------------------------\n",
      "Epoch 5, loss: 0.18873638287186623\n",
      "Test_acc: 0.5\n",
      "--------------------------\n",
      "Epoch 6, loss: 0.17851299419999123\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 7, loss: 0.16922875493764877\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 8, loss: 0.16107673197984695\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 9, loss: 0.15404684096574783\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 10, loss: 0.14802725985646248\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 11, loss: 0.14287303388118744\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 12, loss: 0.1384414155036211\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 13, loss: 0.13460607267916203\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 14, loss: 0.1312607266008854\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 15, loss: 0.12831821851432323\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 16, loss: 0.12570794858038425\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 17, loss: 0.12337299063801765\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 18, loss: 0.12126746959984303\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 19, loss: 0.11935433000326157\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 20, loss: 0.11760355532169342\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 21, loss: 0.11599067784845829\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 22, loss: 0.11449568346142769\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 23, loss: 0.11310208030045033\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 24, loss: 0.11179621517658234\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 25, loss: 0.11056671850383282\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 26, loss: 0.1094040796160698\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 27, loss: 0.10830028168857098\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 28, loss: 0.10724855586886406\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 29, loss: 0.10624313727021217\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 30, loss: 0.1052791029214859\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 31, loss: 0.10435222089290619\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 32, loss: 0.10345886647701263\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 33, loss: 0.10259587690234184\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 34, loss: 0.10176053084433079\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 35, loss: 0.10095042362809181\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 36, loss: 0.10016347281634808\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 37, loss: 0.09939785301685333\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 38, loss: 0.098651934415102\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 39, loss: 0.09792428836226463\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 40, loss: 0.09721364639699459\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 41, loss: 0.09651889279484749\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 42, loss: 0.09583901055157185\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 43, loss: 0.09517310746014118\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 44, loss: 0.09452036395668983\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 45, loss: 0.0938800685107708\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 46, loss: 0.09325156174600124\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 47, loss: 0.09263424947857857\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 48, loss: 0.09202760085463524\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 49, loss: 0.09143111668527126\n",
      "Test_acc: 0.5333333333333333\n",
      "--------------------------\n",
      "Epoch 50, loss: 0.09084436297416687\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 51, loss: 0.09026693738996983\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 52, loss: 0.08969846740365028\n",
      "Test_acc: 0.5666666666666667\n",
      "--------------------------\n",
      "Epoch 53, loss: 0.08913861028850079\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 54, loss: 0.08858705312013626\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 55, loss: 0.08804351277649403\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 56, loss: 0.08750772476196289\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 57, loss: 0.08697944693267345\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 58, loss: 0.08645843341946602\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 59, loss: 0.08594449236989021\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 60, loss: 0.08543741516768932\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 61, loss: 0.08493702113628387\n",
      "Test_acc: 0.6\n",
      "--------------------------\n",
      "Epoch 62, loss: 0.08444313704967499\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 63, loss: 0.08395560085773468\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 64, loss: 0.08347426354885101\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 65, loss: 0.08299897983670235\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 66, loss: 0.08252961002290249\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 67, loss: 0.08206603676080704\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 68, loss: 0.0816081278026104\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 69, loss: 0.08115577697753906\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 70, loss: 0.08070887438952923\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 71, loss: 0.08026730641722679\n",
      "Test_acc: 0.6333333333333333\n",
      "--------------------------\n",
      "Epoch 72, loss: 0.07983098179101944\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 73, loss: 0.07939981482923031\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 74, loss: 0.07897369377315044\n",
      "Test_acc: 0.6666666666666666\n",
      "--------------------------\n",
      "Epoch 75, loss: 0.07855254411697388\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 76, loss: 0.07813627645373344\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 77, loss: 0.07772481068968773\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 78, loss: 0.07731806486845016\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 79, loss: 0.07691597566008568\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 80, loss: 0.07651845179498196\n",
      "Test_acc: 0.7\n",
      "--------------------------\n",
      "Epoch 81, loss: 0.07612544111907482\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 82, loss: 0.07573685608804226\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 83, loss: 0.07535265013575554\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 84, loss: 0.07497275061905384\n",
      "Test_acc: 0.7333333333333333\n",
      "--------------------------\n",
      "Epoch 85, loss: 0.07459708210080862\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 86, loss: 0.07422559335827827\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 87, loss: 0.07385822758078575\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 88, loss: 0.07349492330104113\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 89, loss: 0.0731356181204319\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 90, loss: 0.0727802598848939\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 91, loss: 0.07242879830300808\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 92, loss: 0.07208118122071028\n",
      "Test_acc: 0.7666666666666667\n",
      "--------------------------\n",
      "Epoch 93, loss: 0.07173734251409769\n",
      "Test_acc: 0.8\n",
      "--------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, loss: 0.07139723561704159\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 95, loss: 0.07106082048267126\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 96, loss: 0.07072803843766451\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 97, loss: 0.07039883732795715\n",
      "Test_acc: 0.8\n",
      "--------------------------\n",
      "Epoch 98, loss: 0.07007318176329136\n",
      "Test_acc: 0.8333333333333334\n",
      "--------------------------\n",
      "Epoch 99, loss: 0.0697510102763772\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 100, loss: 0.06943229492753744\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 101, loss: 0.06911696959286928\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 102, loss: 0.06880500260740519\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 103, loss: 0.068496348336339\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 104, loss: 0.06819095741957426\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 105, loss: 0.06788879353553057\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 106, loss: 0.06758981756865978\n",
      "Test_acc: 0.8666666666666667\n",
      "--------------------------\n",
      "Epoch 107, loss: 0.0672939782962203\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 108, loss: 0.06700124125927687\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 109, loss: 0.06671155989170074\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 110, loss: 0.06642490718513727\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 111, loss: 0.06614123564213514\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 112, loss: 0.06586050800979137\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 113, loss: 0.06558268237859011\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 114, loss: 0.06530772428959608\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 115, loss: 0.06503560300916433\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 116, loss: 0.06476627010852098\n",
      "Test_acc: 0.9\n",
      "--------------------------\n",
      "Epoch 117, loss: 0.06449970323592424\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 118, loss: 0.06423585396260023\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 119, loss: 0.06397469528019428\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 120, loss: 0.06371619179844856\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 121, loss: 0.06346031185239553\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 122, loss: 0.06320700887590647\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 123, loss: 0.06295627169311047\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 124, loss: 0.06270804442465305\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 125, loss: 0.062462314032018185\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 126, loss: 0.062219033017754555\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 127, loss: 0.061978185549378395\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 128, loss: 0.06173973437398672\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 129, loss: 0.06150364130735397\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 130, loss: 0.06126988586038351\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 131, loss: 0.06103843078017235\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 132, loss: 0.060809263959527016\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 133, loss: 0.06058233231306076\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 134, loss: 0.06035762373358011\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 135, loss: 0.06013510562479496\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 136, loss: 0.05991474352777004\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 137, loss: 0.05969652719795704\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 138, loss: 0.05948041472584009\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 139, loss: 0.059266386553645134\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 140, loss: 0.059054408222436905\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 141, loss: 0.058844465762376785\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 142, loss: 0.05863652750849724\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 143, loss: 0.058430563658475876\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 144, loss: 0.058226559311151505\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 145, loss: 0.05802448187023401\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 146, loss: 0.05782431084662676\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 147, loss: 0.0576260257512331\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 148, loss: 0.05742959305644035\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 149, loss: 0.057234992273151875\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 150, loss: 0.05704221595078707\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 151, loss: 0.05685121938586235\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 152, loss: 0.05666199326515198\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 153, loss: 0.05647451523691416\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 154, loss: 0.0562887629494071\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 155, loss: 0.05610471125692129\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 156, loss: 0.05592234432697296\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 157, loss: 0.0557416332885623\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 158, loss: 0.05556256324052811\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 159, loss: 0.05538512021303177\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 160, loss: 0.05520927160978317\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 161, loss: 0.0550350034609437\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 162, loss: 0.054862307384610176\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 163, loss: 0.05469114426523447\n",
      "Test_acc: 0.9333333333333333\n",
      "--------------------------\n",
      "Epoch 164, loss: 0.05452151037752628\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 165, loss: 0.05435337871313095\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 166, loss: 0.05418673437088728\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 167, loss: 0.054021554067730904\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 168, loss: 0.053857832215726376\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 169, loss: 0.05369554739445448\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 170, loss: 0.05353467632085085\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 171, loss: 0.05337520316243172\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 172, loss: 0.05321711581200361\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 173, loss: 0.053060390055179596\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 174, loss: 0.05290501844137907\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 175, loss: 0.05275098513811827\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 176, loss: 0.0525982566177845\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 177, loss: 0.05244683939963579\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 178, loss: 0.05229670740664005\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 179, loss: 0.05214785039424896\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 180, loss: 0.052000246942043304\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 181, loss: 0.05185388680547476\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 182, loss: 0.05170875135809183\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 183, loss: 0.0515648303553462\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 184, loss: 0.0514221116900444\n",
      "Test_acc: 0.9666666666666667\n",
      "--------------------------\n",
      "Epoch 185, loss: 0.05128058046102524\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 186, loss: 0.05114021524786949\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 187, loss: 0.051001012325286865\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 188, loss: 0.0508629409596324\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 189, loss: 0.05072600767016411\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 190, loss: 0.05059019848704338\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 191, loss: 0.05045548640191555\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 192, loss: 0.050321875140070915\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 193, loss: 0.05018933489918709\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 194, loss: 0.05005786381661892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 195, loss: 0.04992745537310839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 196, loss: 0.04979807883501053\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 197, loss: 0.04966974165290594\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 198, loss: 0.04954242426902056\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 199, loss: 0.04941611457616091\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 200, loss: 0.049290805123746395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 201, loss: 0.049166472628712654\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 202, loss: 0.04904312454164028\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 203, loss: 0.04892073106020689\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 204, loss: 0.04879929404705763\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 205, loss: 0.04867880046367645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 206, loss: 0.04855923913419247\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 207, loss: 0.048440598882734776\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 208, loss: 0.04832286946475506\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 209, loss: 0.04820604622364044\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 210, loss: 0.04809010960161686\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 211, loss: 0.04797505959868431\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 212, loss: 0.04786087851971388\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 213, loss: 0.047747560776770115\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 214, loss: 0.04763508960604668\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 215, loss: 0.04752346687018871\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 216, loss: 0.0474126823246479\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 217, loss: 0.04730272572487593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 218, loss: 0.04719358030706644\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 219, loss: 0.04708524979650974\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 220, loss: 0.046977708116173744\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 221, loss: 0.046870965510606766\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 222, loss: 0.04676500242203474\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 223, loss: 0.046659816056489944\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 224, loss: 0.046555391512811184\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 225, loss: 0.04645173158496618\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 226, loss: 0.046348825097084045\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 227, loss: 0.04624664504081011\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 228, loss: 0.046145214699208736\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 229, loss: 0.04604450333863497\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 230, loss: 0.04594451282173395\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 231, loss: 0.045845234766602516\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 232, loss: 0.04574666079133749\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 233, loss: 0.04564878437668085\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 234, loss: 0.04555159714072943\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 235, loss: 0.04545509163290262\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 236, loss: 0.045359269715845585\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 237, loss: 0.04526410344988108\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 238, loss: 0.04516960680484772\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 239, loss: 0.04507576581090689\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 240, loss: 0.044982570223510265\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 241, loss: 0.04489001724869013\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 242, loss: 0.04479810781776905\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 243, loss: 0.04470681864768267\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 244, loss: 0.04461614973843098\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 245, loss: 0.04452611040323973\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 246, loss: 0.04443667363375425\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 247, loss: 0.044347839429974556\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 248, loss: 0.04425961058586836\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 249, loss: 0.04417197220027447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 250, loss: 0.044084908440709114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 251, loss: 0.043998440727591515\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 252, loss: 0.04391253925859928\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 253, loss: 0.043827205896377563\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 254, loss: 0.04374244436621666\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 255, loss: 0.04365824069827795\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 256, loss: 0.04357459116727114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 257, loss: 0.043491488322615623\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 258, loss: 0.043408920988440514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 259, loss: 0.043326896615326405\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 260, loss: 0.04324540589004755\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 261, loss: 0.043164435774087906\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 262, loss: 0.04308399651199579\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 263, loss: 0.043004062958061695\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 264, loss: 0.04292465187609196\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 265, loss: 0.04284573998302221\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 266, loss: 0.04276733938604593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 267, loss: 0.042689427733421326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 268, loss: 0.042612009681761265\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 269, loss: 0.042535084299743176\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 270, loss: 0.04245864413678646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 271, loss: 0.0423826826736331\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 272, loss: 0.042307195253670216\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 273, loss: 0.04223217722028494\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 274, loss: 0.0421576201915741\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 275, loss: 0.042083533480763435\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 276, loss: 0.04200989939272404\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 277, loss: 0.041936714202165604\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 278, loss: 0.041863986290991306\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 279, loss: 0.04179169982671738\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 280, loss: 0.041719854809343815\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 281, loss: 0.041648441925644875\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 282, loss: 0.04157746955752373\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 283, loss: 0.04150692094117403\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 284, loss: 0.04143680352717638\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 285, loss: 0.04136709962040186\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 286, loss: 0.04129782039672136\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 287, loss: 0.04122895374894142\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 288, loss: 0.04116049408912659\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 289, loss: 0.04109244793653488\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 290, loss: 0.04102479945868254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 291, loss: 0.04095755238085985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 292, loss: 0.040890694595873356\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 293, loss: 0.040824233554303646\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 294, loss: 0.040758166462183\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 295, loss: 0.040692479349672794\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 296, loss: 0.04062717128545046\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 297, loss: 0.040562248788774014\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 298, loss: 0.04049769788980484\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 299, loss: 0.04043351951986551\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 300, loss: 0.04036970995366573\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 301, loss: 0.040306271985173225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 302, loss: 0.04024319350719452\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 303, loss: 0.04018046986311674\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 304, loss: 0.040118103846907616\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 305, loss: 0.04005609406158328\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 306, loss: 0.03999443957582116\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 307, loss: 0.03993312222883105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 308, loss: 0.039872155059129\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 309, loss: 0.03981153108179569\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 310, loss: 0.03975124144926667\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 311, loss: 0.03969129454344511\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 312, loss: 0.03963167825713754\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 313, loss: 0.039572385139763355\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 314, loss: 0.0395134249702096\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 315, loss: 0.039454787503927946\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 316, loss: 0.039396482054144144\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 317, loss: 0.03933848813176155\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 318, loss: 0.03928081039339304\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 319, loss: 0.039223446510732174\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 320, loss: 0.039166401606053114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 321, loss: 0.039109662640839815\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 322, loss: 0.03905322263017297\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 323, loss: 0.03899709461256862\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 324, loss: 0.03894126648083329\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 325, loss: 0.038885737769305706\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 326, loss: 0.03883049916476011\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 327, loss: 0.03877555998042226\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 328, loss: 0.03872091369703412\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 329, loss: 0.038666556123644114\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 330, loss: 0.0386124849319458\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 331, loss: 0.038558701518923044\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 332, loss: 0.03850520169362426\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 333, loss: 0.03845197660848498\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 334, loss: 0.03839903511106968\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 335, loss: 0.03834636742249131\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 336, loss: 0.03829397866502404\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 337, loss: 0.038241852074861526\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 338, loss: 0.03819000208750367\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 339, loss: 0.03813841659575701\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 340, loss: 0.038087102584540844\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 341, loss: 0.03803604608401656\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 342, loss: 0.037985255010426044\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 343, loss: 0.0379347144626081\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 344, loss: 0.03788443887606263\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 345, loss: 0.037834418937563896\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 346, loss: 0.03778465045616031\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 347, loss: 0.03773513110354543\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 348, loss: 0.03768586413934827\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 349, loss: 0.03763684118166566\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 350, loss: 0.037588071543723345\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 351, loss: 0.037539539858698845\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 352, loss: 0.03749124659225345\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 353, loss: 0.03744320431724191\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 354, loss: 0.03739539487287402\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 355, loss: 0.037347821053117514\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 356, loss: 0.037300472147762775\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 357, loss: 0.03725337469950318\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 358, loss: 0.03720649937167764\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 359, loss: 0.037159846629947424\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 360, loss: 0.03711343090981245\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 361, loss: 0.03706724522635341\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 362, loss: 0.03702127141878009\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 363, loss: 0.036975531373173\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 364, loss: 0.036930006463080645\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 365, loss: 0.036884702276438475\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 366, loss: 0.03683961182832718\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 367, loss: 0.036794747691601515\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 368, loss: 0.03675009263679385\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 369, loss: 0.03670565178617835\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 370, loss: 0.03666142327710986\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 371, loss: 0.036617396865040064\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 372, loss: 0.03657358651980758\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 373, loss: 0.03652997827157378\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 374, loss: 0.03648658422753215\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 375, loss: 0.03644339134916663\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 376, loss: 0.03640039125457406\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 377, loss: 0.03635760257020593\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 378, loss: 0.0363150117918849\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 379, loss: 0.03627262031659484\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 380, loss: 0.036230423022061586\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 381, loss: 0.03618842549622059\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 382, loss: 0.03614661982282996\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 383, loss: 0.036105002742260695\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 384, loss: 0.03606357332319021\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 385, loss: 0.036022343672811985\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 386, loss: 0.03598130075260997\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 387, loss: 0.0359404431656003\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 388, loss: 0.035899775102734566\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 389, loss: 0.03585928911343217\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 390, loss: 0.035818991251289845\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 391, loss: 0.03577886475250125\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 392, loss: 0.035738930106163025\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 393, loss: 0.035699169617146254\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 394, loss: 0.03565958747640252\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 395, loss: 0.0356201883405447\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 396, loss: 0.03558096336200833\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 397, loss: 0.035541911609470844\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 398, loss: 0.035503033082932234\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 399, loss: 0.03546432685106993\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 400, loss: 0.03542578825727105\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 401, loss: 0.03538742894306779\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 402, loss: 0.03534923028200865\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 403, loss: 0.03531120624393225\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 404, loss: 0.03527334099635482\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 405, loss: 0.035235646180808544\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 406, loss: 0.035198114812374115\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 407, loss: 0.03516074409708381\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 408, loss: 0.03512354753911495\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 409, loss: 0.035086496733129025\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 410, loss: 0.03504961961880326\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 411, loss: 0.03501288779079914\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 412, loss: 0.03497632406651974\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 413, loss: 0.034939910750836134\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 414, loss: 0.034903660882264376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 415, loss: 0.03486755723133683\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 416, loss: 0.034831615164875984\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 417, loss: 0.03479582257568836\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 418, loss: 0.034760179463773966\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 419, loss: 0.03472469002008438\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 420, loss: 0.034689351450651884\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 421, loss: 0.03465416468679905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 422, loss: 0.03461912088096142\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 423, loss: 0.034584226086735725\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 424, loss: 0.0345494719222188\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 425, loss: 0.03451487235724926\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 426, loss: 0.034480408765375614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 427, loss: 0.03444609651342034\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 428, loss: 0.03441191930323839\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 429, loss: 0.034377888310700655\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 430, loss: 0.0343439974822104\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 431, loss: 0.03431024495512247\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 432, loss: 0.034276632592082024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 433, loss: 0.03424314921721816\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 434, loss: 0.034209814853966236\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 435, loss: 0.034176611341536045\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 436, loss: 0.03414354659616947\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 437, loss: 0.034110613632947206\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 438, loss: 0.03407781524583697\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 439, loss: 0.03404514491558075\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 440, loss: 0.03401261428371072\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 441, loss: 0.03398020705208182\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 442, loss: 0.03394793579354882\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 443, loss: 0.03391578933224082\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 444, loss: 0.033883774653077126\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 445, loss: 0.03385189222171903\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 446, loss: 0.03382013039663434\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 447, loss: 0.033788496162742376\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 448, loss: 0.033756992779672146\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 449, loss: 0.03372560581192374\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 450, loss: 0.03369434829801321\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 451, loss: 0.03366321278735995\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 452, loss: 0.03363220160827041\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 453, loss: 0.03360130963847041\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 454, loss: 0.033570545725524426\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 455, loss: 0.03353988938033581\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 456, loss: 0.03350936621427536\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 457, loss: 0.03347895201295614\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 458, loss: 0.03344865795224905\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 459, loss: 0.033418478444218636\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 460, loss: 0.033388426061719656\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 461, loss: 0.033358484506607056\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 462, loss: 0.033328657038509846\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 463, loss: 0.033298940397799015\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 464, loss: 0.033269339706748724\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 465, loss: 0.03323985496535897\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 466, loss: 0.03321048337966204\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 467, loss: 0.03318122075870633\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 468, loss: 0.0331520764157176\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 469, loss: 0.033123028464615345\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 470, loss: 0.033094107173383236\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 471, loss: 0.033065286464989185\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 472, loss: 0.0330365770496428\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 473, loss: 0.03300797566771507\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 474, loss: 0.03297947999089956\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 475, loss: 0.03295109001919627\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 476, loss: 0.03292280342429876\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 477, loss: 0.03289463231340051\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 478, loss: 0.0328665585257113\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 479, loss: 0.03283859835937619\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 480, loss: 0.03281073085963726\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 481, loss: 0.03278297046199441\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 482, loss: 0.032755312509834766\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 483, loss: 0.03272775420919061\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 484, loss: 0.03270029416307807\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 485, loss: 0.032672947738319635\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 486, loss: 0.032645690720528364\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 487, loss: 0.03261853428557515\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 488, loss: 0.03259148681536317\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 489, loss: 0.03256453387439251\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 490, loss: 0.032537673600018024\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 491, loss: 0.03251091670244932\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 492, loss: 0.03248425014317036\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 493, loss: 0.0324576823040843\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 494, loss: 0.032431211322546005\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 495, loss: 0.03240483347326517\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 496, loss: 0.03237855713814497\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 497, loss: 0.03235236741602421\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 498, loss: 0.03232627175748348\n",
      "Test_acc: 1.0\n",
      "--------------------------\n",
      "Epoch 499, loss: 0.0323002771474421\n",
      "Test_acc: 1.0\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 训练部分\n",
    "for epoch in range(epoch):  #数据集级别的循环，每个epoch循环一次数据集\n",
    "    for step, (x_train, y_train) in enumerate(train_db):  #batch级别的循环 ，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
    "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
    "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
    "        # 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "        # 实现梯度更新 w1 = w1 - lr * w1_grad    b = b - lr * b_grad\n",
    "        w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
    "        b1.assign_sub(lr * grads[1])  # 参数b自更新\n",
    "\n",
    "    # 每个epoch，打印loss信息\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all/4))\n",
    "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备\n",
    "\n",
    "    # 测试部分\n",
    "    # total_correct为预测对的样本个数, total_number为测试的总样本数，将这两个变量都初始化为0\n",
    "    total_correct, total_number = 0, 0\n",
    "    for x_test, y_test in test_db:\n",
    "        # 使用更新后的参数进行预测\n",
    "        y = tf.matmul(x_test, w1) + b1\n",
    "        y = tf.nn.softmax(y)\n",
    "        pred = tf.argmax(y, axis=1)  # 返回y中最大值的索引，即预测的分类\n",
    "        # 将pred转换为y_test的数据类型\n",
    "        pred = tf.cast(pred, dtype=y_test.dtype)\n",
    "        # 若分类正确，则correct=1，否则为0，将bool型的结果转换为int型\n",
    "        correct = tf.cast(tf.equal(pred, y_test), dtype=tf.int32)\n",
    "        # 将每个batch的correct数加起来\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        # 将所有batch中的correct数加起来\n",
    "        total_correct += int(correct)\n",
    "        # total_number为测试的总样本数，也就是x_test的行数，shape[0]返回变量的行数\n",
    "        total_number += x_test.shape[0]\n",
    "    # 总的准确率等于total_correct/total_number\n",
    "    acc = total_correct / total_number\n",
    "    test_acc.append(acc)\n",
    "    print(\"Test_acc:\", acc)\n",
    "    print(\"--------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3ca7e086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制 loss 曲线\n",
    "plt.title('Loss Function Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Loss')  # y轴变量名称\n",
    "plt.plot(train_loss_results, label=\"$Loss$\")  # 逐点画出trian_loss_results值并连线，连线图标是Loss\n",
    "plt.legend()  # 画出曲线图标\n",
    "plt.show()  # 画出图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d40e26a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制 Accuracy 曲线\n",
    "plt.title('Acc Curve')  # 图片标题\n",
    "plt.xlabel('Epoch')  # x轴变量名称\n",
    "plt.ylabel('Acc')  # y轴变量名称\n",
    "plt.plot(test_acc, label=\"$Accuracy$\")  # 逐点画出test_acc值并连线，连线图标是Accuracy\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edaca1a",
   "metadata": {},
   "source": [
    "# 需要重点复习训练部分代码，背下来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfaf990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
